{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data' # Use the data folder provided in Tutorial 02 - Intro to Pandas.\n",
    "EBOLA = DATA_FOLDER + \"/ebola\"\n",
    "GUINEA = EBOLA + \"/guinea_data\"\n",
    "LIBERIA = EBOLA + \"/liberia_data\"\n",
    "SIERRA_LEONE = EBOLA + \"/sl_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the data\n",
    "\n",
    "First of all, we will split the cleaning in three parts, one for each country, since the files were consistent for each country. Hence, we start by loading all of the data from each folder into three different DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# A few helper functions\n",
    "\n",
    "\"\"\"\n",
    "Returns a pandas dataframe from a folder full of \n",
    "csv files.\n",
    "\"\"\"\n",
    "def create_folder_data_frame(FOLDER):\n",
    "    list_ = []\n",
    "    for file_ in glob.glob(FOLDER + \"/*.csv\"):\n",
    "        df = pd.read_csv(file_, index_col=None)\n",
    "        list_.append(df)\n",
    "    return pd.concat(list_)\n",
    "\n",
    "\"\"\"\n",
    "Sets a new column \"Country\" that becomes part of a multi index\n",
    "afterwards, and returns the indexed frame.\n",
    "\"\"\"\n",
    "def index_and_country(data_frame, country):\n",
    "    data_frame[\"Country\"] = country\n",
    "    data_frame.set_index(['Date', 'Description', 'Country'], inplace=True)\n",
    "    return data_frame\n",
    "    \n",
    "\"\"\"\n",
    "We preprocess each Country one after the other and set their indices\n",
    "to a MultiIndex of Date, Description and Country, with each column\n",
    "mapped to one of these indices.\n",
    "\"\"\"\n",
    "def preprocess(FOLDER, name, columns=None):\n",
    "    frame = create_folder_data_frame(FOLDER)\n",
    "    if columns:\n",
    "        frame.rename(columns=columns, inplace=True)\n",
    "    frame.Date = pd.to_datetime(frame.Date)\n",
    "    frame = index_and_country(frame, name)\n",
    "    return frame\n",
    "    \n",
    "preprocessed_guinea = preprocess(GUINEA, \"Guinea\")\n",
    "\n",
    "liberia_column_mapping = {\"Variable\": \"Description\", \"National\": \"Totals\", 'Unnamed: 18': \"Unknown\"}\n",
    "preprocessed_liberia = preprocess(LIBERIA, \"Liberia\", liberia_column_mapping)\n",
    "\n",
    "sierra_column_mapping = {\"variable\": \"Description\", \"National\": \"Totals\", 'Unnamed: 18': \"Unknown\", \"date\": \"Date\"}\n",
    "preprocessed_sierra = preprocess(SIERRA_LEONE, \"Sierra Leone\", sierra_column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cleaning up\n",
    "\n",
    "Now that we have our three DataFrames, we need to merge them into a single one in order to query it to get the daily average deaths and cases. What we need to do in order to merge them, is to clean each DataFrame, and create a consistency inbetween columns after choosing which columns to use.\n",
    "\n",
    "After taking a look at the data, we decided to deliver the daily average of new cases and new deaths as a sum of the following categories: the suspected, probables, and confirmed cases and deaths, hence we only kept for each DataFrame the right columns. When the \"right\" columns are not perfectly defined, we explain which one we chose and why. The following set of columns are preselected after looking at the data, the other columns were either inconsistent or made no sense whatsoever.\n",
    "\n",
    "#### 1.2.1 Guinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The columns that are of interest to us, mapped\n",
    "# to their centralized names for the final DataFrame\n",
    "interest = { 'New cases of suspects': 'new_suspected',\n",
    "        'New cases of probables': 'new_probable',\n",
    "        'New cases of confirmed': 'new_confirmed',\n",
    "        'Total deaths (confirmed + probables + suspects)': 'deaths',\n",
    "        'Total deaths of suspects': 'death_suspected',\n",
    "        'Total deaths of probables': 'death_probable',\n",
    "        'Total deaths of confirmed': 'death_confirmed' }\n",
    "\n",
    "    \n",
    "# We start by keeping only the interesting columns for the task\n",
    "interest_indices = preprocessed_guinea.index.get_level_values('Description').isin(interest.keys())\n",
    "guinea_df = preprocessed_guinea[interest_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the first assumption we made regarding the data of Guinea, there are no missing values in the totals of deaths, but there are some NaN values in the New cases. We figured that of these new cases, NaN meant that there were either no data for the day, or that the data was not entered. It seems more plausible that people did not fill the data when there was no new cases, so we went with that explanation and changed all NaN values to 0. Also, since the number of cases and deaths can only be discrete we converted the dataframe to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guinea_df = guinea_df.fillna(0).astype(int)\n",
    "guinea_df = guinea_df.sort_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By verifying that the sum of cities and the total column are the same, we can see that there are some discrepencies. We then decided to remove the rows where the discrepencies are too high, namely 10% compared to the Totals column. Otherwise we keep the row, and we finally decided to keep mean of both columns, as they were quite similar, and it was impossible to split them apart and choose one. So the mean is the most obvious merging tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keeping only the interesting columns, namely one for the Total\n",
    "# and one for the total of cities, to filter out unreliable data points\n",
    "guinea_df['Cities_total'] = guinea_df.sum(1) - guinea_df.Totals\n",
    "guinea_df = guinea_df[['Cities_total', 'Totals']]\n",
    "equality = sum(guinea_df['Cities_total'] == guinea_df['Totals']) == 0\n",
    "print(\"Does the sum of cities equal the Totals columm ? {}\".format(equality))\n",
    "\n",
    "# Now we want to remove the rows for which the total of cities \n",
    "# diverges from the Totals column of at least 10%\n",
    "guinea_df = guinea_df[np.abs(guinea_df.Cities_total - guinea_df.Totals) <= (0.1 * guinea_df.Totals)]\n",
    "guinea_df['Totals'] = guinea_df.mean(1)\n",
    "guinea_df = guinea_df.Totals\n",
    "\n",
    "# Converting the indices to columns, and readjusting the indices\n",
    "guinea_df = guinea_df.unstack('Description', fill_value=0)\n",
    "guinea_df.rename(columns=interest, inplace=True)\n",
    "\n",
    "display(HTML(guinea_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the total deaths, we can see that there is a row where we have missing data, we assumed here that the person forgot to write the measurements, hence we decided to fill the row with the mean of the previous and next measurements. That would simulate a linear increase and seems reasonable for one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filling the row with the mean of the previous and next measurements\n",
    "# We also remove the cumulative sum, in order to get the daily data\n",
    "for c in guinea_df.columns:\n",
    "    if \"death_\" in c:\n",
    "        guinea_df[c].loc[('2014-09-26', 'Guinea')] = int((guinea_df[c].loc[('2014-09-24', 'Guinea')] + guinea_df[c].loc[('2014-09-30', 'Guinea')]) / 2)\n",
    "        guinea_df[c] = guinea_df[c] - guinea_df[c][0]\n",
    "        guinea_df[c][1:] = guinea_df[c][1:].copy().as_matrix() - guinea_df[c][:-1].copy().as_matrix()\n",
    "        \n",
    "# And finally, since the data can get negative because of the previous\n",
    "# computations we made, we can set them back to 0\n",
    "guinea_df[guinea_df < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can take a look at the sum of all 3 categories into one. But first, we need to choose how to process the fact that we can pick either from the deaths column or the sum of the three other categories of deaths. Let's take a look at the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea_df['new_cases'] = guinea_df[['new_suspected', 'new_confirmed', 'new_probable']].sum(axis=1)\n",
    "guinea_df['deaths_summed'] = guinea_df[['death_confirmed', 'death_probable', 'death_suspected']].sum(axis=1)\n",
    "# We reverse the cumulative sum on the deaths column to get a daily value\n",
    "guinea_df['deaths'] = guinea_df['deaths'] - guinea_df['deaths'][0]\n",
    "guinea_df['deaths'][1:] = guinea_df['deaths'][1:].copy().as_matrix() - guinea_df['deaths'][:-1].copy().as_matrix()\n",
    "\n",
    "display(HTML(guinea_df[['deaths', 'deaths_summed']].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the column deaths and the column death_summed, we can clearly see that they are about the same, besides a few discrepencies and two or three clearly wrong values. Hence, we decided to only retain the deaths_summed column that seemed more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We now have a clean DataFrame to work with:\n",
    "guinea_df = guinea_df[['new_cases', 'deaths_summed']]\n",
    "guinea_df.rename(columns={'deaths_summed':'deaths'}, inplace=True)\n",
    "\n",
    "display(HTML(guinea_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Sierra Leone\n",
    "\n",
    "Now for Sierra Leone, most of our assumptions still hold, so we will explain less of our choices, and quickly explain through comments. If there is something new, we will explain it more thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting all values to integer and replacing all NaNs by 0\n",
    "preprocessed_sierra = preprocessed_sierra.apply(pd.to_numeric,errors='coerce')\n",
    "preprocessed_sierra = preprocessed_sierra.fillna(0).astype(int)\n",
    "\n",
    "# We wanted to check if the 'Police traning School' and 'Police training School' were the same\n",
    "# As the sum of 'Police traning School' equals 0, we decided to drop this one\n",
    "# which must have been a typo\n",
    "print('Sum for Police traning: ', preprocessed_sierra['Police traning School'].sum())\n",
    "print('Sum for Police training: ', preprocessed_sierra['Police training School'].sum())\n",
    "preprocessed_sierra.drop('Police traning School', axis =1, inplace=True)\n",
    "\n",
    "\n",
    "# Just like before, keeping only the interesting descriptions\n",
    "interest = ['death_suspected',\n",
    "               'new_probable',\n",
    "               'new_suspected',\n",
    "               'death_confirmed',\n",
    "               'new_confirmed',\n",
    "               'death_probable']\n",
    "indices_interest = preprocessed_sierra.index.get_level_values('Description').isin(interest)\n",
    "preprocessed_sierra = preprocessed_sierra[indices_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we did the columns a bit differently, same as before, we compute the sum of cities and compare it to the Totals column. We then remove outliers by computing the difference between the maximum of both columns and the mean. This is a precise metric to decide wether a point is an outlier or not, it is quite robust against the main problem: columns containing 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_sierra['Cities_total'] = preprocessed_sierra.sum(axis=1) - preprocessed_sierra['Totals']\n",
    "preprocessed_sierra['Mean'] = (preprocessed_sierra['Totals'] + preprocessed_sierra['Cities_total']) / 2\n",
    "max_of_both = preprocessed_sierra[['Cities_total', 'Totals']].max(1)\n",
    "preprocessed_sierra = preprocessed_sierra[np.abs(max_of_both - preprocessed_sierra.Mean) <= (0.1 * preprocessed_sierra.Mean)]\n",
    "\n",
    "# We remove rows with 0 in the death columns\n",
    "# Indeed these are cumulative columns and so a 0 makes no sense.\n",
    "# Before we changed one row, but here there are too many rows to do this.\n",
    "sierra = preprocessed_sierra['Mean'].unstack('Description', fill_value=0)\n",
    "sierra = sierra[(sierra['death_suspected'] != 0) & (sierra['death_probable'] != 0) & (sierra['death_confirmed'] != 0) ]\n",
    "\n",
    "#The data give us only the cumulated total for the deaths. So we computed the dayly numbers of deaths\n",
    "deaths = ['death_suspected', 'death_probable', 'death_confirmed']\n",
    "for c in sierra.columns:\n",
    "    if c in deaths:\n",
    "        sierra[c] = sierra[c] - sierra[c][0]\n",
    "        sierra[c][1:] = sierra[c][1:].copy().as_matrix() - sierra[c][:-1].copy().as_matrix()\n",
    "\n",
    "#Set the minimum value to 0    \n",
    "sierra = sierra.clip(lower=0)\n",
    "# We sum the 'suspected, probable and confirmed' columns\n",
    "sierra['deaths'] = sierra[['death_suspected', 'death_probable', 'death_confirmed']].sum(1)\n",
    "sierra['new_cases'] = sierra[['new_confirmed', 'new_probable', 'new_suspected']].sum(1)\n",
    "sierra = sierra[['deaths', 'new_cases']]\n",
    "\n",
    "# Here is the final dataframe for Sierra Leone\n",
    "display(HTML(sierra.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Liberia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly like the other countries, we keep the obvious columns, and analyze the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keeping only the interesting columns\n",
    "interest_liberia = { 'New Case/s (Suspected)': 'new_suspected', \n",
    "        'New Case/s (Probable)': 'new_probable', \n",
    "        'New case/s (confirmed)': 'new_confirmed', \n",
    "        'Total death/s in suspected cases': 'death_suspected', \n",
    "        'Total death/s in probable cases': 'death_probable', \n",
    "        'Total death/s in confirmed cases': 'death_confirmed',\n",
    "        'Total death/s in confirmed, probable, suspected cases': 'total_death'}\n",
    "\n",
    "interest_liberia_indices = preprocessed_liberia.index.get_level_values('Description').isin(interest_liberia.keys())\n",
    "liberia_df = preprocessed_liberia[interest_liberia_indices]\n",
    "\n",
    "# Sames assumptions as before\n",
    "liberia_df = liberia_df.fillna(0).astype(int).sort_index(0)\n",
    "\n",
    "# Keeping two columns for the sum of cities and the total\n",
    "liberia_df['Cities_total'] = liberia_df.sum(1) - liberia_df.Totals\n",
    "liberia_df = liberia_df[['Cities_total', 'Totals']]\n",
    "\n",
    "# Let's check out all the data that diverges of 10% between the two columns\n",
    "liberia_df_limit = liberia_df[np.abs(liberia_df.Cities_total - liberia_df.Totals) > (0.1 * liberia_df.Totals)]\n",
    "\n",
    "# We choose which of the deaths rows to remove, since \n",
    "# when counting in thousands the 10% make sense\n",
    "liberia_df_total = liberia_df_limit.xs('Total death/s in confirmed, probable, suspected cases', level='Description')\n",
    "total_date = liberia_df_total.index.get_level_values(0)\n",
    "\n",
    "# Now for the others, we decided to remove the row when either \n",
    "# of the columns contain a 0 (no data), we can give any confidence\n",
    "# to an average of the two in these special cases\n",
    "liberia_df_zero = liberia_df_limit[(liberia_df_limit.values == 0)]\n",
    "zero_date = liberia_df_zero.index.get_level_values(0)\n",
    "\n",
    "# Dropping the rows\n",
    "to_drop = total_date.append(zero_date).strftime('%Y-%m-%d')\n",
    "liberia_df.drop(to_drop, level=0, inplace=True)\n",
    "\n",
    "# We compute the mean between the total and the total of cities\n",
    "liberia_df['Mean'] = liberia_df.mean(1)\n",
    "liberia_df = liberia_df.Mean\n",
    "\n",
    "# Converting the indices to columns, and readjusting the indices\n",
    "liberia_df = liberia_df.unstack('Description', fill_value=0)\n",
    "liberia_df.columns.rename('', inplace=True)\n",
    "liberia_df = liberia_df.rename(columns=interest_liberia)\n",
    "\n",
    "# We sum the 'suspected, probable and confirmed' columns\n",
    "liberia_df['total_death_sum'] = liberia_df[['death_suspected','death_probable','death_confirmed']].sum(1)\n",
    "liberia_df['new_cases_sum'] = liberia_df[['new_probable','new_suspected','new_confirmed']].sum(1)\n",
    "\n",
    "# Moreover it allows us to compare the total death sum with the one present directly in the data\n",
    "liberia_df_diff = liberia_df[liberia_df['total_death_sum'] != liberia_df['total_death']]\n",
    "\n",
    "display(HTML(liberia_df_diff[['total_death','total_death_sum']].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous dataframe shows only the data points that diverge between the two columns. We obviously decided to keep the total_death column that has a lot more information. And moreover, here it makes no sense to take the mean of both as the results would make no sense since we mean with a non existing value (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We keep our final columns\n",
    "liberia_df = liberia_df[['total_death', 'new_cases_sum']]\n",
    "\n",
    "# Reverting the cumulative sum for deaths\n",
    "liberia_df['total_death'] = liberia_df['total_death'] - liberia_df['total_death'][0]\n",
    "liberia_head = liberia_df['total_death'][1:].copy().as_matrix()\n",
    "liberia_tail = liberia_df['total_death'][:-1].copy().as_matrix()\n",
    "liberia_df['total_death'][1:] = liberia_head - liberia_tail\n",
    "\n",
    "# Moreover we have noticed that the last 6 rows of the new cases seem to be \n",
    "# cumulative instead of daily that is why we also revert the cumulative sum for them\n",
    "liberia_df['new_cases_sum'][-6:] = liberia_df['new_cases_sum'][-6:] - liberia_df['new_cases_sum'][-6:][0]\n",
    "liberia_head = liberia_df['new_cases_sum'][-6:][1:].copy().as_matrix()\n",
    "liberia_tail = liberia_df['new_cases_sum'][-6:][:-1].copy().as_matrix()\n",
    "liberia_df['new_cases_sum'][-6:][1:] = liberia_head - liberia_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the distribution of total deaths to make sure that the column is right, and we see a few problems, after decumulating the sums, we have huge positive and negative values due to the fact that there were zeros before decumulating. To fix this, we bring these values back to NaN, and they will not be used when computing the final mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(liberia_df['total_death'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liberia_df[((liberia_df < 0) | (liberia_df > 1700))] = np.nan\n",
    "\n",
    "# Quick renaming of the columns\n",
    "liberia_df.rename(columns={'new_cases_sum': 'new_cases', 'total_death': 'deaths'}, inplace=True)\n",
    "\n",
    "# Here is the final dataframe for Liberia\n",
    "display(HTML(liberia_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Merging and answering the questions\n",
    "\n",
    "Here is the final DataFrame, of all deaths and new_cases sorted by date and country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ebola_df = pd.concat([guinea_df, sierra, liberia_df]).sort_index(0)\n",
    "display(HTML(ebola_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the daily average per month of deaths and new cases of ebola by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Here are the daily averages by month for each country:\")\n",
    "grouped_df = ebola_df.groupby([ebola_df.index.get_level_values('Date').month, \n",
    "                             ebola_df.index.get_level_values('Country')])\n",
    "display(HTML(grouped_df.mean().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"We can see it as a beautiful histogram instead:\")\n",
    "\n",
    "color_choices = ['#f44336','#01579b']\n",
    "\n",
    "bar_guinea = grouped_df.mean().xs('Guinea', level='Country').plot(kind='bar',title =\"Guinea\", color=color_choices)\n",
    "bar_guinea.set_xlabel(\"Month\")\n",
    "bar_guinea.set_ylabel(\"Daily Average\")\n",
    "\n",
    "bar_sl = grouped_df.mean().xs('Sierra Leone', level='Country').plot(kind='bar', title =\"Sierra Leone\", color=color_choices)\n",
    "bar_sl.set_xlabel(\"Month\")\n",
    "bar_sl.set_ylabel(\"Daily Average\")\n",
    "\n",
    "bar_liberia = grouped_df.mean().xs('Liberia', level='Country').plot(kind='bar', title =\"Liberia\", color=color_choices)\n",
    "bar_liberia.set_xlabel(\"Month\")\n",
    "bar_liberia.set_ylabel(\"Daily Average\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are satisfying and make sense. Apart from two months in Liberia, we can see that in November and December, its average of cases is 0. This is due to the fact that we had no data for these two months in the columns that we chose. There was some data in the column \"Newly reported deaths\" but it was highly inconsistent with the data of the other columns, and we assumed it was not counting the exact same thing. Hence we decided to keep the total at zero and say we have no data for these two months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "microbiome = DATA_FOLDER + '/microbiome/'\n",
    "#Create dataframe for each file except metadata\n",
    "def create_df(name, group, sample):\n",
    "    df = pd.read_excel(microbiome + name +'.xls', header=None )\n",
    "    #Set the index with the names columns\n",
    "    df.columns = ['Name', 'Count']\n",
    "    df.set_index('Name', inplace=True)\n",
    "    \n",
    "    #Add multiple leves columns to the dataframe\n",
    "    col = pd.MultiIndex.from_product([[group], [sample]], names=['Group', 'Sample'])\n",
    "    df.columns = col\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Create dataframe for the metadata file\n",
    "metadata = pd.read_excel(DATA_FOLDER+'/microbiome/metadata.xls')\n",
    "metadata.fillna('unknown', inplace=True)\n",
    "\n",
    "#From the metadata dataframe creates an unique dataframe with the others files\n",
    "data = []\n",
    "for index, row in metadata.iterrows():\n",
    "    data.append(create_df(row['BARCODE'], row['GROUP'], row['SAMPLE']))\n",
    "result = pd.concat(data, axis=0)\n",
    "#Make the index unique by summing rows with same index\n",
    "result = result.groupby(result.index).sum()\n",
    "result.fillna('unknown', inplace=True)\n",
    "print(\"Index unique: \", result.index.is_unique)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic_df = pd.read_excel(DATA_FOLDER +'/titanic.xls')\n",
    "display(HTML(titanic_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "\n",
    "We start by describing the columns, where we can find some information about each column, namely the data types for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titanic_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are a few columns that are obviously integers, when we count something for example, or when we want to categorize something (we will see more about that later), hence the integer columns are: *pclass*, *survived*, *sibsp* and *parch*. We could argue that a few other columns should be integers, for example the *age*, the *ticket* number or even the *boat* number. But after taking a look at the data, each of these are strings or double for a reason. Some of these columns will be categorized later on, we will talk about that later.\n",
    "\n",
    "We chose to let the *age* column as is, in order to create a new one containing the *age* categorized. As for the object ones, they are all strings, that pandas interprets as objects. The *age* is a float since for young infants it is written as a continuous age between 0 and 1 for example and sometimes \"half year\" is counted (18.5 etc.). The *fare* is also a a float to account for the cents. As for the body, it is counted as a float mainly because of the NaNs which prevents the column to be integers.\n",
    "\n",
    "The only value ranges that are of any interest to us are the ones from the integer and float columns since the others will be categorized. We can take a look at the description given by pandas to see the value ranges directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pclass*, *survived*, and *body* are categories and hence not really interesting for value ranges. So we will focus on the other few. The *age* tells us that there were young infants on the boat but also old people, it ranges from 0.1667 which is considered as an infant between 0 and 1, up to 80 years old. *sibsp* or the number of siblings/spouses aboard ranges from 0 to 8, meaning that there were single travelers, but also big families of up to 8 people. We can say exactly the same for *parch* which is the number of parents/children aboard, it ranges from 0 to 9, that confirms the bit about big families being onboard. And the *fare* ranges from 0 british pounds to about 512 british pounds.\n",
    "\n",
    "Now onto categorizing: *survived* should be categorical: a simple yes or no (boolean), as well as *pclass* which seems to be the ticket's class, in the other columns we have the *sex* that is also a category, the *cabin*, the *body*, the *embarked* column which is the port of origin, the *boat* number, the *home.dest*, the *ticket* and *name* can also be considered as categorical. We decided to change the discretization of some of the categories to have them make more sense.\n",
    "\n",
    "Let's now apply all of the changes that we talked about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorizing\n",
    "to_categorize = ['pclass', 'survived', 'name', 'sex', 'cabin', 'body', 'embarked', 'ticket', 'boat', 'home.dest']\n",
    "titanic_df[to_categorize] = titanic_df[to_categorize].apply(lambda c: c.astype('category'))\n",
    "\n",
    "categorization_map = {'pclass': {1: 'First class', 2: 'Second class', 3: 'Third class'},\n",
    "                      'survived': {1: True, 0: False},\n",
    "                      'sex': {\"female\": \"Female\", \"male\": \"Male\"},\n",
    "                      'embarked': {\"C\": \"Cherbourg\", \"Q\": \"Queenstown\", \"S\": \"Southampton\" }\n",
    "                     }\n",
    "\n",
    "# Modifying some columns\n",
    "for column in categorization_map.keys():\n",
    "    titanic_df[column] = titanic_df[column].map(categorization_map[column])\n",
    "    \n",
    "# Let's create a new column for ages cut in 4 quantiles\n",
    "titanic_df['age_block'] = pd.qcut(titanic_df.age, 4)\n",
    "\n",
    "print(\"Here is the Titanic DataFrame that we will work with:\")\n",
    "display(HTML(titanic_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Attributes we want to plot, with corresponding titles\n",
    "to_plot = {'pclass': 'Travel Class',\n",
    "        'embarked': 'Embarkation port',\n",
    "        'sex': 'Sex',\n",
    "        'age_block': 'Age interval' }\n",
    "\n",
    "# We plot the number of passengers for each selected attribute's category (or intervals).\n",
    "for c in to_plot:\n",
    "    titanic_hist = titanic_df[c].value_counts().sort_index().plot(kind='bar',title = to_plot[c], rot=0)\n",
    "    titanic_hist.set_ylabel(\"Number of passengers\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cabin floor is represented by the letter preceding the variable cabin. We drop the NaN values to compute better statistics. And here is the proportion of passengers by cabin floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_series = titanic_df['cabin'].dropna()\n",
    "# Retaining only the first letter\n",
    "cabin_series = cabin_series.apply(lambda x: x[0])\n",
    "print(\"Here are the cabin floors: {}\".format(', '.join(cabin_series.unique())))\n",
    "\n",
    "# Let's count them !\n",
    "cabin_series.value_counts().plot.pie(figsize=(7, 7),  autopct='%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we start by dropping the rows with NaN values to compute better statistics. We then group the pclass and survived columns to finally compute the proportion of passengers that survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_survived_df = titanic_df[['pclass', 'survived']].dropna()\n",
    "class_survived_df = pd.DataFrame(class_survived_df.groupby(['pclass', 'survived'])\n",
    "                                 .size()).unstack('survived')\n",
    "class_survived_df.columns = class_survived_df.columns.droplevel()\n",
    "class_survived_df.rename(columns={True: \"Survived\", False: \"Died\"}, inplace=True)\n",
    "\n",
    "plt.axis('equal')\n",
    "for pclass in class_survived_df.index:\n",
    "    plt.title(pclass)\n",
    "    class_survived_df.loc[pclass].plot.pie(figsize=(5, 5),\n",
    "                                            autopct='%1.1f%%', startangle=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, you really wanted to be in First class at the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We groupe our dataframe with the asked attributes (survived, travel class and sex)\n",
    "titanic_grouped = titanic_df.groupby(['pclass', 'sex', 'survived'])\n",
    "\n",
    "# We compute the size of each group\n",
    "titanic_grouped = titanic_grouped.size()\n",
    "\n",
    "# We pivot the survived category in order to have a new level of column\n",
    "titanic_grouped = titanic_grouped.unstack('survived')\n",
    "\n",
    "# We compute the proportion of the passengers that survied by travel class and sex\n",
    "titanic_grouped['prop_true'] = titanic_grouped[True] * 100 / titanic_grouped.sum(1)\n",
    "\n",
    "print(\"Here the proportion of the passengers that survived by travel class and sex:\")\n",
    "display(HTML(titanic_grouped.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the dataframe of proportion of passengers that survived \n",
    "# And we change the indices in order to have a correct legend for our plot\n",
    "titanic_prop = titanic_grouped[['prop_true']].unstack('sex')\n",
    "titanic_prop.columns = titanic_prop.columns.droplevel().rename('')\n",
    "titanic_prop.index.rename('', inplace = True)\n",
    "\n",
    "# We plot our dataframe\n",
    "color_cs = ['#4286f4', '#ff5e23']\n",
    "titanic_class_sex = titanic_prop.plot(kind='bar', rot=0, color=color_cs)\n",
    "titanic_class_sex.set_title('Proportion of the passengers that survived by travel class and sex')\n",
    "titanic_class_sex.set_ylabel(\"Proportion in percentage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep our interesting columns\n",
    "titanic_agecat = titanic_df[['age', 'pclass', 'sex', 'survived']]\n",
    "\n",
    "# We create 2 equally populated age categories\n",
    "titanic_agecat['age_interval'] = pd.qcut(titanic_agecat.age, 2)\n",
    "\n",
    "# We groupe our dataframe with the asked attributes (survived, travel class and sex)\n",
    "titanic_agecat_gb = titanic_agecat.groupby(['age_interval', 'pclass', 'sex', 'survived'])\n",
    "\n",
    "# We compute the size of each group\n",
    "titanic_agecat_gb = titanic_agecat_gb.size()\n",
    "\n",
    "# We pivot the survived category in order to have a new level of column\n",
    "titanic_agecat_gb = titanic_agecat_gb.unstack('survived')\n",
    "\n",
    "# We compute the proportion of the passengers that survied by age category, travel class and sex\n",
    "titanic_agecat_gb['proportion_survived'] = titanic_agecat_gb[True] * 100 / titanic_agecat_gb.sum(1)\n",
    "\n",
    "# We only keep the proportion and we delete our general column name\n",
    "titanic_agecat_gb = titanic_agecat_gb[['proportion_survived']]\n",
    "titanic_agecat_gb.columns.rename('', inplace=True)\n",
    "\n",
    "display(HTML(titanic_agecat_gb.to_html()))\n",
    "print('We can finally verify that the DataFrame has a unique index : ', titanic_agecat_gb.index.is_unique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
