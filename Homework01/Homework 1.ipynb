{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data' # Use the data folder provided in Tutorial 02 - Intro to Pandas.\n",
    "EBOLA = DATA_FOLDER + \"/ebola\"\n",
    "GUINEA = EBOLA + \"/guinea_data\"\n",
    "LIBERIA = EBOLA + \"/liberia_data\"\n",
    "SIERRA_LEONE = EBOLA + \"/sl_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the data\n",
    "\n",
    "First of all, we will split the cleaning in three parts, one for each country, since the files were consistent for each country. Hence, we start by loading all of the data from each folder into three different DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# A few helper functions\n",
    "\n",
    "\"\"\"\n",
    "Returns a pandas dataframe from a folder full of \n",
    "csv files.\n",
    "\"\"\"\n",
    "def create_folder_data_frame(FOLDER):\n",
    "    list_ = []\n",
    "    for file_ in glob.glob(FOLDER + \"/*.csv\"):\n",
    "        df = pd.read_csv(file_, index_col=None)\n",
    "        list_.append(df)\n",
    "    return pd.concat(list_)\n",
    "\n",
    "\"\"\"\n",
    "Sets a new column \"Country\" that becomes part of a multi index\n",
    "afterwards, and returns the indexed frame.\n",
    "\"\"\"\n",
    "def index_and_country(data_frame, country):\n",
    "    data_frame[\"Country\"] = country\n",
    "    data_frame.set_index(['Date', 'Description', 'Country'], inplace=True)\n",
    "    return data_frame\n",
    "    \n",
    "\"\"\"\n",
    "We preprocess each Country one after the other and set their indices\n",
    "to a MultiIndex of Date, Description and Country, with each column\n",
    "mapped to one of these indices.\n",
    "\"\"\"\n",
    "def preprocess(FOLDER, name, columns=None):\n",
    "    frame = create_folder_data_frame(FOLDER)\n",
    "    if columns:\n",
    "        frame.rename(columns=columns, inplace=True)\n",
    "    frame.Date = pd.to_datetime(frame.Date)\n",
    "    frame = index_and_country(frame, name)\n",
    "    return frame\n",
    "    \n",
    "preprocessed_guinea = preprocess(GUINEA, \"Guinea\")\n",
    "\n",
    "liberia_column_mapping = {\"Variable\": \"Description\", \"National\": \"Totals\", 'Unnamed: 18': \"Unknown\"}\n",
    "preprocessed_liberia = preprocess(LIBERIA, \"Liberia\", liberia_column_mapping)\n",
    "\n",
    "sierra_column_mapping = {\"variable\": \"Description\", \"National\": \"Totals\", 'Unnamed: 18': \"Unknown\", \"date\": \"Date\"}\n",
    "preprocessed_sierra = preprocess(SIERRA_LEONE, \"Sierra Leone\", sierra_column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cleaning up\n",
    "\n",
    "Now that we have our three DataFrames, we need to merge them into a single one in order to query it to get the daily average deaths and cases. What we need to do in order to merge them, is to clean each DataFrame, and create a consistency inbetween columns after choosing which columns to use.\n",
    "\n",
    "After taking a look at the data, we decided to deliver the daily average of new cases and new deaths as a sum of the following categories: the suspected, probables, and confirmed cases and deaths, hence we only kept for each DataFrame the right columns. When the \"right\" columns are not perfectly defined, we explain which one we chose and why. The following set of columns are preselected after looking at the data, the other columns were either inconsistent or made no sense whatsoever.\n",
    "\n",
    "#### 1.2.1 Guinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The columns that are of interest to us, mapped\n",
    "# to their centralized names for the final DataFrame\n",
    "interest = { 'New cases of suspects': 'new_suspected',\n",
    "        'New cases of probables': 'new_probable',\n",
    "        'New cases of confirmed': 'new_confirmed',\n",
    "        'Total deaths (confirmed + probables + suspects)': 'deaths',\n",
    "        'Total deaths of suspects': 'death_suspected',\n",
    "        'Total deaths of probables': 'death_probable',\n",
    "        'Total deaths of confirmed': 'death_confirmed' }\n",
    "\n",
    "    \n",
    "# We start by keeping only the interesting columns for the task\n",
    "interest_indices = preprocessed_guinea.index.get_level_values('Description').isin(interest.keys())\n",
    "guinea_df = preprocessed_guinea[interest_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the first assumption we made regarding the data of Guinea, there are no missing values in the totals of deaths, but there are some NaN values in the New cases. We figured that of these new cases, NaN meant that there were either no data for the day, or that the data was not entered. It seems more plausible that people did not fill the data when there was no new cases, so we went with that explanation and changed all NaN values to 0. Also, since the number of cases and deaths can only be discrete we converted the dataframe to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea_df = guinea_df.fillna(0).astype(int)\n",
    "guinea_df = guinea_df.sort_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By verifying that the sum of cities and the total column are the same, we can see that there are some discrepencies. We then decided to remove the rows where the discrepencies are too high, namely 10% compared to the Totals column. Otherwise we keep the row, and we finally decided to keep mean of both columns, as they were quite similar, and it was impossible to split them apart and choose one. So the mean is the most obvious merging tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keeping only the interesting columns, namely one for the Total\n",
    "# and one for the total of cities, to filter out unreliable data points\n",
    "guinea_df['Cities_total'] = guinea_df.sum(1) - guinea_df.Totals\n",
    "guinea_df = guinea_df[['Cities_total', 'Totals']]\n",
    "equality = sum(guinea_df['Cities_total'] == guinea_df['Totals']) == 0\n",
    "print(\"Does the sum of cities equal the Totals columm ? {}\".format(equality))\n",
    "\n",
    "# Now we want to remove the rows for which the total of cities \n",
    "# diverges from the Totals column of at least 10%\n",
    "guinea_df = guinea_df[np.abs(guinea_df.Cities_total - guinea_df.Totals) <= (0.1 * guinea_df.Totals)]\n",
    "guinea_df['Totals'] = guinea_df.mean(1)\n",
    "guinea_df = guinea_df.Totals\n",
    "\n",
    "# Converting the indices to columns, and readjusting the indices\n",
    "guinea_df = guinea_df.unstack('Description', fill_value=0)\n",
    "guinea_df.rename(columns=interest, inplace=True)\n",
    "\n",
    "display(HTML(guinea_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the total deaths, we can see that there is a row where we have missing data, we assumed here that the person forgot to write the measurements, hence we decided to fill the row with the mean of the previous and next measurements. That would simulate a linear increase and seems reasonable for one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filling the row with the mean of the previous and next measurements\n",
    "# We also remove the cumulative sum, in order to get the daily data\n",
    "for c in guinea_df.columns:\n",
    "    if \"death_\" in c:\n",
    "        guinea_df[c].loc[('2014-09-26', 'Guinea')] = int((guinea_df[c].loc[('2014-09-24', 'Guinea')] + guinea_df[c].loc[('2014-09-30', 'Guinea')]) / 2)\n",
    "        guinea_df[c] = guinea_df[c] - guinea_df[c][0]\n",
    "        guinea_df[c][1:] = guinea_df[c][1:].copy().as_matrix() - guinea_df[c][:-1].copy().as_matrix()\n",
    "        \n",
    "# And finally, since the data can get negative because of the previous\n",
    "# computations we made, we can set them back to 0\n",
    "guinea_df[guinea_df < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can take a look at the sum of all 3 categories into one. But first, we need to choose how to process the fact that we can pick either from the deaths column or the sum of the three other categories of deaths. Let's take a look at the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea_df['new_cases'] = guinea_df[['new_suspected', 'new_confirmed', 'new_probable']].sum(axis=1)\n",
    "guinea_df['deaths_summed'] = guinea_df[['death_confirmed', 'death_probable', 'death_suspected']].sum(axis=1)\n",
    "# We reverse the cumulative sum on the deaths column to get a daily value\n",
    "guinea_df['deaths'] = guinea_df['deaths'] - guinea_df['deaths'][0]\n",
    "guinea_df['deaths'][1:] = guinea_df['deaths'][1:].copy().as_matrix() - guinea_df['deaths'][:-1].copy().as_matrix()\n",
    "\n",
    "display(HTML(guinea_df[['deaths', 'deaths_summed']].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the column deaths and the column death_summed, we can clearly see that they are about the same, besides a few discrepencies and two or three clearly wrong values. Hence, we decided to only retain the deaths_summed column that seemed more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We now have a clean DataFrame to work with:\n",
    "guinea_df = guinea_df[['new_cases', 'deaths_summed']]\n",
    "guinea_df.rename(columns={'deaths_summed':'deaths'}, inplace=True)\n",
    "\n",
    "display(HTML(guinea_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Sierra Leone\n",
    "\n",
    "Now for Sierra Leone, most of our assumptions still hold, so we will explain less of our choices, and quickly explain through comments. If there is something new, we will explain it more thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting all values to integer and replacing all NaNs by 0\n",
    "preprocessed_sierra = preprocessed_sierra.apply(pd.to_numeric,errors='coerce')\n",
    "preprocessed_sierra = preprocessed_sierra.fillna(0).astype(int)\n",
    "\n",
    "# We wanted to check if the 'Police traning School' and 'Police training School' were the same\n",
    "# As the sum of 'Police traning School' equals 0, we decided to drop this one\n",
    "# which must have been a typo\n",
    "print('Sum for Police traning: ', preprocessed_sierra['Police traning School'].sum())\n",
    "print('Sum for Police training: ', preprocessed_sierra['Police training School'].sum())\n",
    "preprocessed_sierra.drop('Police traning School', axis =1, inplace=True)\n",
    "\n",
    "\n",
    "# Just like before, keeping only the interesting descriptions\n",
    "interest = ['death_suspected',\n",
    "               'new_probable',\n",
    "               'new_suspected',\n",
    "               'death_confirmed',\n",
    "               'new_confirmed',\n",
    "               'death_probable']\n",
    "indices_interest = preprocessed_sierra.index.get_level_values('Description').isin(interest)\n",
    "preprocessed_sierra = preprocessed_sierra[indices_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we did the columns a bit differently, same as before, we compute the sum of cities and compare it to the Totals column. We then remove outliers by computing the difference between the maximum of both columns and the mean. This is a precise metric to decide wether a point is an outlier or not, it is quite robust against the main problem: columns containing 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_sierra['Cities_total'] = preprocessed_sierra.sum(axis=1) - preprocessed_sierra['Totals']\n",
    "preprocessed_sierra['Mean'] = (preprocessed_sierra['Totals'] + preprocessed_sierra['Cities_total']) / 2\n",
    "max_of_both = preprocessed_sierra[['Cities_total', 'Totals']].max(1)\n",
    "preprocessed_sierra = preprocessed_sierra[np.abs(max_of_both - preprocessed_sierra.Mean) <= (0.1 * preprocessed_sierra.Mean)]\n",
    "\n",
    "# We remove rows with 0 in the death columns\n",
    "# Indeed these are cumulative columns and so a 0 makes no sense.\n",
    "# Before we changed one row, but here there are too many rows to do this.\n",
    "sierra = preprocessed_sierra['Mean'].unstack('Description', fill_value=0)\n",
    "sierra = sierra[(sierra['death_suspected'] != 0) & (sierra['death_probable'] != 0) & (sierra['death_confirmed'] != 0) ]\n",
    "\n",
    "#The data give us only the cumulated total for the deaths. So we computed the dayly numbers of deaths\n",
    "deaths = ['death_suspected', 'death_probable', 'death_confirmed']\n",
    "for c in sierra.columns:\n",
    "    if c in deaths:\n",
    "        sierra[c] = sierra[c] - sierra[c][0]\n",
    "        sierra[c][1:] = sierra[c][1:].copy().as_matrix() - sierra[c][:-1].copy().as_matrix()\n",
    "\n",
    "#Set the minimum value to 0    \n",
    "sierra = sierra.clip(lower=0)\n",
    "sierra['deaths'] = sierra['death_suspected'] + sierra['death_probable'] + sierra['death_confirmed']\n",
    "sierra['new_cases'] = sierra['new_confirmed'] + sierra['new_probable'] +sierra['new_suspected']\n",
    "sierra = sierra[['deaths', 'new_cases']]\n",
    "\n",
    "# Here is the final dataframe for Sierra Leone\n",
    "display(HTML(sierra.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Liberia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly like the other countries, we keep the obvious columns, and analyze the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keeping only the interesting columns\n",
    "interest_liberia = { 'New Case/s (Suspected)': 'new_suspected', \n",
    "        'New Case/s (Probable)': 'new_probable', \n",
    "        'New case/s (confirmed)': 'new_confirmed', \n",
    "        'Total death/s in suspected cases': 'death_suspected', \n",
    "        'Total death/s in probable cases': 'death_probable', \n",
    "        'Total death/s in confirmed cases': 'death_confirmed',\n",
    "        'Total death/s in confirmed, probable, suspected cases': 'total_death'}\n",
    "\n",
    "interest_liberia_indices = preprocessed_liberia.index.get_level_values('Description').isin(interest_liberia.keys())\n",
    "liberia_df = preprocessed_liberia[interest_liberia_indices]\n",
    "\n",
    "# Sames assumptions as before\n",
    "liberia_df = liberia_df.fillna(0).astype(int).sort_index(0)\n",
    "\n",
    "# Keeping two columns for the sum of cities and the total\n",
    "liberia_df['Cities_total'] = liberia_df.sum(1) - liberia_df.Totals\n",
    "liberia_df = liberia_df[['Cities_total', 'Totals']]\n",
    "\n",
    "# Let's check out all the data that diverges of 10% between the two columns\n",
    "liberia_df_limit = liberia_df[np.abs(liberia_df.Cities_total - liberia_df.Totals) > (0.1 * liberia_df.Totals)]\n",
    "\n",
    "# We choose which of the deaths rows to remove, since \n",
    "# when counting in thousands the 10% make sense\n",
    "liberia_df_total = liberia_df_limit.xs('Total death/s in confirmed, probable, suspected cases', level='Description')\n",
    "total_date = liberia_df_total.index.get_level_values(0)\n",
    "\n",
    "# Now for the others, we decided to remove the row when either \n",
    "# of the columns contain a 0 (no data), we can give any confidence\n",
    "# to an average of the two in these special cases\n",
    "liberia_df_zero = liberia_df_limit[(liberia_df_limit.values == 0)]\n",
    "zero_date = liberia_df_zero.index.get_level_values(0)\n",
    "\n",
    "# Dropping the rows\n",
    "to_drop = total_date.append(zero_date).strftime('%Y-%m-%d')\n",
    "liberia_df.drop(to_drop, level=0, inplace=True)\n",
    "\n",
    "# We compute the mean between the total and the total of cities\n",
    "liberia_df['Mean'] = liberia_df.mean(1)\n",
    "liberia_df = liberia_df.Mean\n",
    "\n",
    "# Converting the indices to columns, and readjusting the indices\n",
    "liberia_df = liberia_df.unstack('Description', fill_value=0)\n",
    "liberia_df.columns.rename('', inplace=True)\n",
    "liberia_df = liberia_df.rename(columns=interest_liberia)\n",
    "\n",
    "# We sum the 'suspected, probable and confirmed' columns\n",
    "liberia_df['total_death_sum'] = liberia_df[['death_suspected','death_probable','death_confirmed']].sum(1)\n",
    "liberia_df['new_cases_sum'] = liberia_df[['new_probable','new_suspected','new_confirmed']].sum(1)\n",
    "\n",
    "# Moreover it allows us to compare the total death sum with the one present directly in the data\n",
    "liberia_df_diff = liberia_df[liberia_df['total_death_sum'] != liberia_df['total_death']]\n",
    "\n",
    "display(HTML(liberia_df_diff[['total_death','total_death_sum']].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous dataframe shows only the data points that diverge between the two columns. We obviously decided to keep the total_death column that has a lot more information. And moreover, here it makes no sense to take the mean of both as the results would make no sense since we mean with a non existing value (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We keep our final columns\n",
    "liberia_df = liberia_df[['total_death', 'new_cases_sum']]\n",
    "\n",
    "# Reverting the cumulative sum for deaths\n",
    "liberia_df['total_death'] = liberia_df['total_death'] - liberia_df['total_death'][0]\n",
    "liberia_head = liberia_df['total_death'][1:].copy().as_matrix()\n",
    "liberia_tail = liberia_df['total_death'][:-1].copy().as_matrix()\n",
    "liberia_df['total_death'][1:] = liberia_head - liberia_tail\n",
    "\n",
    "# Moreover we have noticed that the last 6 rows of the new cases seem to be \n",
    "# cumulative instead of daily that is why we also revert the cumulative sum for them\n",
    "liberia_df['new_cases_sum'][-6:] = liberia_df['new_cases_sum'][-6:] - liberia_df['new_cases_sum'][-6:][0]\n",
    "liberia_head = liberia_df['new_cases_sum'][-6:][1:].copy().as_matrix()\n",
    "liberia_tail = liberia_df['new_cases_sum'][-6:][:-1].copy().as_matrix()\n",
    "liberia_df['new_cases_sum'][-6:][1:] = liberia_head - liberia_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the distribution of total deaths to make sure that the column is right, and we see a few problems, after decumulating the sums, we have huge positive and negative values due to the fact that there were zeros before decumulating. To fix this, we bring these values back to NaN, and they will not be used when computing the final mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(liberia_df['total_death'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liberia_df[((liberia_df < 0) | (liberia_df > 1700))] = np.nan\n",
    "\n",
    "# Quick renaming of the columns\n",
    "liberia_df.rename(columns={'new_cases_sum': 'new_cases', 'total_death': 'deaths'}, inplace=True)\n",
    "\n",
    "# Here is the final dataframe for Liberia\n",
    "display(HTML(liberia_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Merging and answering the questions\n",
    "\n",
    "Here is the final DataFrame, of all deaths and new_cases sorted by date and country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ebola_df = pd.concat([guinea_df, sierra, liberia_df]).sort_index(0)\n",
    "display(HTML(ebola_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the daily average per month of deaths and new cases of ebola by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Here are the daily averages by month for each country:\")\n",
    "grouped_df = ebola_df.groupby([ebola_df.index.get_level_values('Date').month, \n",
    "                             ebola_df.index.get_level_values('Country')])\n",
    "display(HTML(grouped_df.mean().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"We can see it as a beautiful histogram instead:\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "grouped_df.mean().plot(kind='bar')\n",
    "plt.axhline(0, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are satisfying and make sense. Apart from two months in Liberia, we can see that in November and December, its average of cases is 0. This is due to the fact that we had no data for these two months in the columns that we chose. There was some data in the column \"Newly reported deaths\" but it was highly inconsistent with the data of the other columns, and we assumed it was not counting the exact same thing. Hence we decided to keep the total at zero and say we have no data for these two months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "microbiome = '/microbiome'\n",
    "def create_folder_data_frame(FOLDER):\n",
    "    list_ = []\n",
    "    for file_ in glob.glob(FOLDER + \"\\*.xls\"):\n",
    "        if file_ != FOLDER + '\\metadata.xls':\n",
    "            print(file_)\n",
    "            df = pd.read_excel(file_, index_col=None)\n",
    "            list_.append(df)\n",
    "    return pd.concat(list_)\n",
    "create_folder_data_frame(DATA_FOLDER+microbiome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic_df = pd.read_excel(DATA_FOLDER +'/titanic.xls')\n",
    "display(HTML(titanic_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "\n",
    "We start by describing the columns, where we can find some information about each column, namely the value range for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the columns that are not above are the ones containing something else than numeric data. Let's focus first on the ones that do. All of the above columns are considered as floats, but there are some that probably should not be. For example *survived* should be categorical: a simple yes or no, as well as *pclass* which seems to be the ticket's class, in the other columns we have the *sex* that is also a category, the *cabin*, the *embarked* column which is the port of origin, the *boat* number, the *home.dest*, the *ticket* and *name* can also be considered as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titanic_df.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
